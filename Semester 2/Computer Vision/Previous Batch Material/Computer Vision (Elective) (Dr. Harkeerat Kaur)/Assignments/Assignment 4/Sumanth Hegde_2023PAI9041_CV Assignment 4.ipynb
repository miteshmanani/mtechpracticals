{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Sumanth Hegde\n",
    "2023PAI9041\n",
    "Computer Vision Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5zKkNgGkx2N"
   },
   "source": [
    "To determine the camera parameters using the perspective-n-point (PnP) algorithm, you need to establish correspondences between 3D world points and their respective 2D image points. These correspondences enable the estimation of both the camera's intrinsic parameters (such as focal length and principal point) and its extrinsic parameters (position and orientation in the world).\n",
    "\n",
    "Here's how to proceed:\n",
    "\n",
    "**Gather Correspondences:** Obtain the 3D coordinates of points in the world coordinate system and their corresponding 2D coordinates in the image. These correspondences are stored in arrays, typically named WorldPoints and ImagePoints.\n",
    "\n",
    "**Apply PnP Algorithm:** Utilize the PnP algorithm, which can be implemented using methods like the Direct Linear Transform (DLT) followed by nonlinear optimization techniques (e.g., Levenberg-Marquardt), to estimate the camera's pose and intrinsic parameters.\n",
    "\n",
    "**Minimum Number of Points:** At least 6 non-coplanar correspondences are required to solve for the camera parameters. However, using more points generally improves accuracy.\n",
    "\n",
    "**Implementation:** Implement this using libraries such as OpenCV in Python. OpenCV offers functions like cv2.solvePnP() or cv2.solvePnPRansac() to determine the camera parameters.\n",
    "\n",
    "Python code using OpenCV to solve for the camera parameters, providing the rotation matrix and translation vector representing the camera's pose in the world coordinate system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B2qhsWYYlm6M",
    "outputId": "dc9b31b8-d623-4d5d-fa20-e56b0398fd8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rotation Matrix:\n",
      "[[-0.99105245 -0.13180255 -0.02105061]\n",
      " [ 0.13284074 -0.9893462  -0.05956031]\n",
      " [-0.01297614 -0.06182377  0.99800273]]\n",
      "\n",
      "Translation Vector:\n",
      "[[  3.99871398]\n",
      " [  8.83873972]\n",
      " [-86.8726709 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Define the 3D coordinates of points in the world and their 2D coordinates in the image\n",
    "world_center_x, world_center_y, world_center_z = -10.9, -10.7, 42\n",
    "world_coords = np.array([[world_center_x, world_center_y, world_center_z],\n",
    "                         [5.5, 3.9, 46.8], [14.2, 3.9, 47.0], [22.8, 3.9, 47.4],\n",
    "                         [5.5, 10.6, 44.2], [14.2, 10.6, 43.8], [22.8, 10.6, 44.8],\n",
    "                         [5.5, 17.3, 43], [14.2, 17.3, 42.5], [22.8, 17.3, 44.4]])\n",
    "\n",
    "image_coords = np.array([[6.28, 3.42], [502, 185], [700, 197], [894, 208],\n",
    "                         [491, 331], [695, 342], [896, 353],\n",
    "                         [478, 487], [691, 497], [900, 508]])\n",
    "\n",
    "# Camera intrinsic matrix (assuming simulated data)\n",
    "# values provided (focal_length_x = 1000, focal_length_y = 1000, center_x = 400, center_y = 300)\n",
    "\n",
    "intrinsic_matrix = np.array([[1000, 0, 400],\n",
    "                             [0, 1000, 300],\n",
    "                             [0, 0, 1]])\n",
    "\n",
    "# Distortion coefficients (assuming no distortion)\n",
    "distortion_coeffs = np.zeros((4, 1))\n",
    "\n",
    "# Solve PnP to find rotation and translation vectors\n",
    "is_solved, rot_vec, trans_vec = cv2.solvePnP(world_coords, image_coords, intrinsic_matrix, distortion_coeffs)\n",
    "\n",
    "# Convert rotation vector to rotation matrix\n",
    "rot_matrix, _ = cv2.Rodrigues(rot_vec)\n",
    "\n",
    "# Print results\n",
    "print(\"Rotation Matrix:\")\n",
    "print(rot_matrix)\n",
    "print(\"\\nTranslation Vector:\")\n",
    "print(trans_vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cv6QaqXStV31"
   },
   "source": [
    "**Minimum Number of Points Required:** 6 corresponding points between the 3D world and 2D image are required to accurately estimate the camera parameters.\n",
    "\n",
    "**Camera Location and Orientation:**\n",
    "\n",
    "Location of the Camera Object:\n",
    "\n",
    "X coordinate: 3.99871398\n",
    "\n",
    "Y coordinate: 8.83873972\n",
    "\n",
    "Z coordinate: -86.8726709\n",
    "\n",
    "Orientation of the Camera Object (Direction of camera axes in world coordinates):\n",
    "\n",
    "X axis direction: [-0.99105245 -0.13180255 -0.02105061]\n",
    "\n",
    "Y axis direction: [ 0.13284074 -0.9893462  -0.05956031]\n",
    "\n",
    "Z axis direction: [-0.01297614 -0.06182377  0.99800273]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QrvJwrdJt7AK"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
