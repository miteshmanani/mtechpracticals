{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGP7UH0l1kT0",
        "outputId": "e9b032ff-de27-4156-b734-3be5bebf36f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: [20.5658277]\n",
            "Epoch 2, Loss: [22.29437467]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m9092/9092\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 13ms/step - loss: 2.4132\n",
            "Epoch 2/2\n",
            "\u001b[1m9092/9092\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 13ms/step - loss: 1.8223\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the dataset\n",
        "def load_data(filename):\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    return text\n",
        "\n",
        "# Prepare data\n",
        "def preprocess_data(text):\n",
        "    chars = sorted(set(text))\n",
        "    char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
        "    idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
        "    encoded_text = np.array([char_to_idx[c] for c in text])\n",
        "    return encoded_text, char_to_idx, idx_to_char\n",
        "\n",
        "# Create input-output pairs\n",
        "def create_dataset(encoded_text, seq_length):\n",
        "    X = []\n",
        "    y = []\n",
        "    for i in range(len(encoded_text) - seq_length):\n",
        "        X.append(encoded_text[i:i+seq_length])\n",
        "        y.append(encoded_text[i+seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Define RNN from scratch\n",
        "class SimpleRNN:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.hidden_size = hidden_size\n",
        "        self.Wxh = np.random.randn(hidden_size, input_size) * 0.01\n",
        "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "        self.Why = np.random.randn(output_size, hidden_size) * 0.01\n",
        "        self.bh = np.zeros((hidden_size, 1))\n",
        "        self.by = np.zeros((output_size, 1))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        h = np.zeros((self.hidden_size, 1))\n",
        "        self.last_inputs = inputs\n",
        "        self.last_hs = {0: h}\n",
        "\n",
        "        for t, x in enumerate(inputs):\n",
        "            x = x.reshape(-1, 1)\n",
        "            h = np.tanh(self.Wxh @ x + self.Whh @ h + self.bh)\n",
        "            self.last_hs[t + 1] = h\n",
        "\n",
        "        y = self.Why @ h + self.by\n",
        "        return y, h\n",
        "\n",
        "    def backward(self, dLdy):\n",
        "        n = len(self.last_inputs)\n",
        "        dWxh, dWhh, dWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
        "        dbh, dby = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
        "        dh = np.zeros_like(self.last_hs[0])\n",
        "\n",
        "        for t in reversed(range(n)):\n",
        "            dy = dLdy\n",
        "            dWhy += dy @ self.last_hs[t + 1].T\n",
        "            dby += dy\n",
        "            dh = self.Why.T @ dy + dh\n",
        "            dhraw = (1 - self.last_hs[t + 1] ** 2) * dh\n",
        "            dbh += dhraw\n",
        "            dWxh += dhraw @ self.last_inputs[t].reshape(1, -1)\n",
        "            dWhh += dhraw @ self.last_hs[t].T\n",
        "            dh = self.Whh.T @ dhraw\n",
        "\n",
        "        return dWxh, dWhh, dWhy, dbh, dby\n",
        "\n",
        "    def update_weights(self, dWxh, dWhh, dWhy, dbh, dby, lr=0.01):\n",
        "        self.Wxh -= lr * dWxh\n",
        "        self.Whh -= lr * dWhh\n",
        "        self.Why -= lr * dWhy\n",
        "        self.bh -= lr * dbh\n",
        "        self.by -= lr * dby\n",
        "\n",
        "# Train RNN\n",
        "\n",
        "def train_rnn(rnn, X, y, epochs=2, lr=0.01):\n",
        "    for epoch in range(epochs):\n",
        "        loss = 0\n",
        "        for i in range(len(X)):\n",
        "            inputs = [np.eye(len(char_to_idx))[x] for x in X[i]]\n",
        "            target = np.eye(len(char_to_idx))[y[i]]\n",
        "\n",
        "            out, _ = rnn.forward(inputs)\n",
        "            probs = np.exp(out) / np.sum(np.exp(out))\n",
        "            loss += -np.log(probs[np.argmax(target)])\n",
        "\n",
        "            dLdy = probs - target.reshape(-1, 1)\n",
        "            dWxh, dWhh, dWhy, dbh, dby = rnn.backward(dLdy)\n",
        "            rnn.update_weights(dWxh, dWhh, dWhy, dbh, dby, lr)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {loss / len(X)}\")\n",
        "\n",
        "# Comparison with TensorFlow\n",
        "\n",
        "def train_tf_model(X, y, vocab_size, seq_length):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, 50, input_length=seq_length),\n",
        "        tf.keras.layers.SimpleRNN(100, return_sequences=False),\n",
        "        tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "    model.fit(X, y, epochs=2, batch_size=64)\n",
        "    return model\n",
        "\n",
        "# Main workflow\n",
        "filename = 'NextWordPrediction_1661-0.txt'\n",
        "text = load_data(filename)\n",
        "encoded_text, char_to_idx, idx_to_char = preprocess_data(text)\n",
        "\n",
        "seq_length = 25\n",
        "X, y = create_dataset(encoded_text, seq_length)\n",
        "\n",
        "# Train custom RNN\n",
        "vocab_size = len(char_to_idx)\n",
        "rnn = SimpleRNN(vocab_size, 100, vocab_size)\n",
        "train_rnn(rnn, X, y)\n",
        "\n",
        "# Train TensorFlow RNN\n",
        "model = train_tf_model(X, y, vocab_size, seq_length)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}