{
 "cells": [
  {
   "cell_type": "raw",
   "id": "9bc6c6c7",
   "metadata": {},
   "source": [
    "Sumanth Hegde\n",
    "2023PAI9041\n",
    "Deep Learning Assignment 1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b6739b59",
   "metadata": {},
   "source": [
    "Answer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f1131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convolve_and_pool(input_image, weight_matrix):\n",
    "    ih, iw = input_image.shape\n",
    "    wh, ww = weight_matrix.shape\n",
    "    output_height = ih - wh + 1\n",
    "    output_width = iw - ww + 1\n",
    "    output = np.zeros((output_height, output_width))\n",
    "    for i in range(output_height):\n",
    "        for j in range(output_width):\n",
    "            output[i, j] = np.sum(input_image[i:i+wh, j:j+ww] * weight_matrix)\n",
    "    average_pooling = np.mean(output)\n",
    "    return output, average_pooling\n",
    "\n",
    "I = np.array([[1, 2, 2], \n",
    "              [1, 3, 4], \n",
    "              [1, 0, 0]])\n",
    "\n",
    "W = np.array([[1, -1], \n",
    "              [1, -1], \n",
    "              [0, 1]])\n",
    "convolved_output, average_pooling_result = convolve_and_pool(I, W)\n",
    "\n",
    "print(\"Convolved Output:\\n\", convolved_output)\n",
    "print(\"Average Pooling Result:\", average_pooling_result)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b8652cee",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b7e261ea",
   "metadata": {},
   "source": [
    "Answer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726186ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Keras (TensorFlow runs on backend)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess data - we will normalize the data so that all values lie in the range [0,1]\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Define CNN architecture\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=128, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8380ea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140f52c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without using Keras or TensorFlow\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist.data, mnist.target\n",
    "\n",
    "# Normalize and reshape data\n",
    "X = X / 255.0\n",
    "X = X.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y = y.astype(int)\n",
    "num_classes = 10\n",
    "y_one_hot = np.eye(num_classes)[y]\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test = X[:60000], X[60000:]\n",
    "y_train, y_test = y_one_hot[:60000], y_one_hot[60000:]\n",
    "\n",
    "# Define convolution operation\n",
    "def convolution(image, filt, bias):\n",
    "    stride = 1\n",
    "    num_filters, filter_size, _ = filt.shape\n",
    "    image_size = image.shape[0]\n",
    "    output_size = (image_size - filter_size) // stride + 1\n",
    "    output = np.zeros((output_size, output_size, num_filters))\n",
    "    \n",
    "    for f in range(num_filters):\n",
    "        y = 0\n",
    "        for i in range(0, image_size - filter_size + 1, stride):\n",
    "            x = 0\n",
    "            for j in range(0, image_size - filter_size + 1, stride):\n",
    "                output[y, x, f] = np.sum(image[i:i+filter_size, j:j+filter_size] * filt[f]) + bias[f]\n",
    "                x += 1\n",
    "            y += 1\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Define max pooling operation\n",
    "def max_pooling(image, size=2):\n",
    "    image_size = image.shape[0]\n",
    "    output_size = image_size // size\n",
    "    output = np.zeros((output_size, output_size, image.shape[2]))\n",
    "    \n",
    "    for z in range(image.shape[2]):\n",
    "        y = 0\n",
    "        for i in range(0, image_size, size):\n",
    "            x = 0\n",
    "            for j in range(0, image_size, size):\n",
    "                output[y, x, z] = np.max(image[i:i+size, j:j+size, z])\n",
    "                x += 1\n",
    "            y += 1\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Define ReLU activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Define softmax activation function\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # For numerical stability\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# Initialize weights and biases\n",
    "conv1_filters = np.random.randn(3, 3, 1, 32)\n",
    "conv1_biases = np.zeros(32)\n",
    "conv2_filters = np.random.randn(3, 3, 32, 64)\n",
    "conv2_biases = np.zeros(64)\n",
    "fc_weights = np.random.randn(7*7*64, 10) / (7*7*64)\n",
    "fc_biases = np.zeros(10)\n",
    "\n",
    "# Define forward pass through the network\n",
    "def forward_pass(image):\n",
    "    # Convolutional Layer 1\n",
    "    conv1_output = convolution(image, conv1_filters, conv1_biases)\n",
    "    conv1_output = relu(conv1_output)\n",
    "    # Max Pooling Layer 1\n",
    "    pool1_output = max_pooling(conv1_output, size=2)\n",
    "    \n",
    "    # Convolutional Layer 2\n",
    "    conv2_output = convolution(pool1_output, conv2_filters, conv2_biases)\n",
    "    conv2_output = relu(conv2_output)\n",
    "    # Max Pooling Layer 2\n",
    "    pool2_output = max_pooling(conv2_output, size=2)\n",
    "    \n",
    "    # Flatten\n",
    "    flatten_output = pool2_output.reshape((len(image), -1))\n",
    "    \n",
    "    # Fully Connected Layer\n",
    "    fc_output = np.dot(flatten_output, fc_weights) + fc_biases\n",
    "    \n",
    "    # Softmax activation\n",
    "    output = softmax(fc_output)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Define cross-entropy loss function\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    log_likelihood = -np.log(y_pred[range(m), np.argmax(y_true, axis=1)])\n",
    "    loss = np.sum(log_likelihood) / m\n",
    "    return loss\n",
    "\n",
    "# Define accuracy function\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))\n",
    "\n",
    "# Training loop\n",
    "learning_rate = 0.01\n",
    "epochs = 20\n",
    "batch_size = 128\n",
    "num_batches = len(X_train) // batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        batch_X = X_train[start:end]\n",
    "        batch_y = y_train[start:end]\n",
    "        \n",
    "        # Forward pass\n",
    "        output = forward_pass(batch_X)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = cross_entropy_loss(batch_y, output)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grad_fc_output = output - batch_y\n",
    "        grad_fc_weights = np.dot(flatten_output.T, grad_fc_output)\n",
    "        grad_fc_biases = np.sum(grad_fc_output, axis=0)\n",
    "        \n",
    "        grad_pool2_output = np.dot(grad_fc_output, fc_weights.T)\n",
    "        grad_pool2_output = grad_pool2_output.reshape(pool2_output.shape)\n",
    "        \n",
    "        grad_conv2_output = np.zeros(conv2_output.shape)\n",
    "        for z in range(conv2_output.shape[2]):\n",
    "            grad_conv2_output[:, :, z] = np.kron(grad_pool2_output[:, :, z], np.ones((2, 2))) / 4.0\n",
    "        \n",
    "        grad_conv2_filters = np.zeros(conv2_filters.shape)\n",
    "        for f in range(conv2_filters.shape[0]):\n",
    "            for c in range(conv2_filters.shape[2]):\n",
    "                grad_conv2_filters[f, :, :, c] = convolution(pool1_output[:, :, c], grad_conv2_output[:, :, f], np.zeros(1))\n",
    "        \n",
    "        grad_conv2_biases = np.sum(grad_conv2_output, axis=(0, 1))\n",
    "        \n",
    "        grad_pool1_output = np.zeros(pool1_output.shape)\n",
    "        for z in range(pool1_output.shape[2]):\n",
    "            grad_pool1_output[:, :, z] = np.kron(grad_conv2_output[:, :, z], np.ones((2, 2))) / 4.0\n",
    "        \n",
    "        grad_conv1_output = np.zeros(conv1_output.shape)\n",
    "        for z in range(conv1_output.shape[2]):\n",
    "            grad_conv1_output[:, :, z] = convolution(batch_X[:, :, z], grad_pool1_output[:, :, z], np.zeros(1))\n",
    "        \n",
    "        grad_conv1_filters = np.zeros(conv1_filters.shape)\n",
    "        for f in range(conv1_filters.shape[0]):\n",
    "            for c in range(conv1_filters.shape[2]):\n",
    "                grad_conv1_filters[f, :, :, c] = convolution(batch_X[:, :, c], grad_conv1_output[:, :, f], np.zeros(1))\n",
    "        \n",
    "        grad_conv1_biases = np.sum(grad_conv1_output, axis=(0, 1))\n",
    "        \n",
    "        # Update weights and biases\n",
    "        conv1_filters -= learning_rate * grad"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58dcad50",
   "metadata": {},
   "source": [
    "In the above code the model architecture includes:\n",
    "\n",
    "Two Convolutional Layers\n",
    "Two MaxPooling Layers\n",
    "\n",
    "Each convolutional layer is followed by a max-pooling layer\n",
    "\n",
    "\n",
    "\n",
    "General Observations:\n",
    "\n",
    "1) The code that did not use Keras or TensorFlow, took significantly more time to execute (almost 25X times more time than the code which used Keras) :(    :(\n",
    "2) Using the Adam optimizer from Keras or TensorFlow is more convenient and requires less code.\n",
    "3) The code without using Keras or TensorFlow gives us more control over the optimizer and allows us to understand its inner workings better.\n",
    "4) The performance of both the above codes might be similar, but the code without using Keras or TensorFlow requires more fine-tuning of hyperparameters for optimal performance."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d726a44",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f1b73d75",
   "metadata": {},
   "source": [
    "Answer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea99e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the same architecture which was used in Question 2 above\n",
    "\n",
    "\n",
    "\n",
    "from keras.layers import Dropout, BatchNormalization\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# Load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess data\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Define CNN architecture with all regularizers\n",
    "model_regularized = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),  # Batch Normalization\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l1(0.01)),  # L1 regularization\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),  # L1-L2 regularization\n",
    "    Dropout(0.5),  # Dropout layer\n",
    "    BatchNormalization(),  # Batch Normalization\n",
    "    Dense(10, activation='softmax', kernel_regularizer=l1(0.01))  # L1 regularization\n",
    "])\n",
    "\n",
    "# Compile the regularized model\n",
    "model_regularized.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the regularized model with early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "model_regularized.fit(X_train, y_train, epochs=20, batch_size=128, \n",
    "                      validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# Data Augmentation\n",
    "datagen = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, \n",
    "                             height_shift_range=0.1, zoom_range=0.1)\n",
    "datagen.fit(X_train)\n",
    "model_regularized.fit(datagen.flow(X_train, y_train, batch_size=128), epochs=20, \n",
    "                      validation_data=(X_test, y_test), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "66e785e2",
   "metadata": {},
   "source": [
    "In the above code we have used the following 5 regulaization methods:\n",
    "\n",
    "1) L2 Regularization: Applied to the first convolutional layer.\n",
    "2) Batch Normalization: Applied after the first convolutional layer and after the first dense layer.\n",
    "3) L1 Regularization: Applied to the second convolutional layer and the last dense layer.\n",
    "2)&3) together L1-L2 Regularization: Applied to the dense layer.\n",
    "4) Dropout: Applied after the first dense layer.\n",
    "5) Data Augmentation: Implemented after the model is compiled and trained.\n",
    "\n",
    "\n",
    "\n",
    "Also we observe that - After applying each regularization technique, the performance of the regularized model has become better as compared to the original model. Regularization methods like are used to prevent overfitting and improve the generalization of deep learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
